{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder for Collaborative Filtering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Model\n",
    "\n",
    "<br>u $\\epsilon$ {1,...,U}\n",
    "<br>i $\\epsilon$ {1,...,I}\n",
    "<br> User-item interaction matrix: $X \\, \\epsilon \\, \\mathbb{N}^{U \\times I}$ where $x_u=[x_{u1},...,x_{uI}]^T \\, \\epsilon \\, \\mathbb{N}^I$\n",
    "\n",
    "<br>$z_u$: K-dimensional hidden factor for user u\n",
    "<br>$f_{\\theta}(.)$: non-linear function $\\epsilon \\, \\mathbb{R}^I$\n",
    "<br>$\\pi(z_u)$: the probability function from which $x_u$ vector is assumed to be drawn\n",
    "<br>$N_u=\\sum_{i} x_{ui}$: Total number of clicks given the user u\n",
    "\n",
    "$$z_u \\sim N(0,I_k)$$\n",
    "$$\\pi(z_u) \\propto exp(\\, f_{\\theta}(z_u)\\,)$$\n",
    "$$x_u \\sim Mult(N_u, \\pi(z_u))$$\n",
    "\n",
    "Log-likelihood function: $log\\, p_{\\theta}(x_u | z_u)=\\sum_i x_{ui} \\, log \\, \\pi_i(z_u)$\n",
    "For comparison:\n",
    "1. Gaussian Log-likelihood\n",
    "2. Logistic log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variational Inference\n",
    "\n",
    "To learn the generative model we need to estimate the parameter $\\theta$ of the non-linear function $f_{\\theta}(.)$. To do so, the intractable posterior $p(z_u |x_u)$ needs to be calculated for each data point. Variational inference approximates the true posterior to an instrumental posterior $q(z_u)$ with Kullback-Leibler divergence $KL(q(z_u) || p(z_u |x_u))$ where,\n",
    "$$q(z_u) \\sim \\mathcal{N}(\\mu_u,diag\\{\\sigma^2_u\\})$$\n",
    "\n",
    "The objective of variational inference to select $(\\mu_u, \\sigma^2_u)$ so that $KL(q(z_u) || p(z_u |x_u))$ is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.Amortized inference and variational autoencoders\n",
    "\n",
    "The number of variational parameters $(\\mu_u, \\sigma^2_u)$ grows with the number of users and items in the dataset. This is a bottleneck for commercial recommendation systems, since there are millions of users and items. To cope with this issue, variational autoencoders are used which are in fact data-dependent function. This is commonly called the inference model:\n",
    "$$g_{\\phi}(x_u)=[\\mu_{\\phi}(x_u), \\sigma_{\\phi}(x_u)] \\: \\epsilon \\: \\mathbb{R}^{2K}$$\n",
    "Thus the variational distribution becomes also data-dependent:\n",
    "$$q_{\\phi}(z_u |x_u) \\sim \\mathcal{N}(\\mu_{\\phi}(x_u), diag\\{\\sigma^2_{\\phi}(x_u)\\})$$\n",
    "\n",
    "Variational autoencoders ease the new inference problems analyzing user preferences by exploiting the similarity patterns inferred from past experiences.\n",
    "\n",
    "*Learning VAEs:\n",
    "\n",
    "\\begin{align*}\n",
    "KL(q_{\\phi}(z_u |x_u) || p_{\\theta}(z_u |x_u)) &= \\mathbb{E}_q [log\\,q_{\\phi}(z_u |x_u)-log \\, p_{\\theta}(z_u |x_u) ] \\qquad  \\textbf{Eq.(1)} \\\\\n",
    "&=\\mathbb{E}_q [log\\,q_{\\phi}(z_u |x_u)-log \\, p_{\\theta}(z_u ,x_u)+ log\\, p_{\\theta}(x_u)] \\\\\n",
    "&=\\mathbb{E}_q [log\\,q_{\\phi}(z_u |x_u)-log \\, p_{\\theta}(x_u|z_u) -log\\, p(z_u)] +log \\, p_{\\theta}(x_u)\n",
    "\\end{align*}\n",
    "\n",
    "The evidence $p(x_u)$ is a constant term, and KL divergence is a non negative term, we can rearrange the equation as below:\n",
    "\n",
    "$$log\\, p_{\\theta}(x_u)\\geq \\mathbb{E}_q[log\\, p_{\\theta}(x_u|z_u)]-KL(q_{\\phi}(z_u|x_u)||p(z_u)) \\equiv \\mathcal{L}(x_u;\\theta, \\phi) \\qquad \\textbf{Eq.(2)}$$\n",
    "\n",
    "$\\mathcal{L}(x_u;\\theta, \\phi)$ is called evidence lower bound(ELBO).\n",
    "\n",
    "\n",
    "Minimizing KL term in Eq.(1) is the same thing as maximizing ELBO.\n",
    "\n",
    "*Reparameterization trick:\n",
    "\n",
    "We can estimate the ELBO by sampling $z_u \\sim q_{\\phi}$ and make gradient ascent to optimize it. However, we can not take gradient of $q_{\\phi}$ w.r.t. $\\phi$. So we do the reparameterization trick:\n",
    "\n",
    "1. Sample $\\epsilon \\sim \\mathcal{N}(0,I_k)$\n",
    "2. Reparameterize $z_u$ such that $z_u=[\\mu_{\\phi}(x_u),\\epsilon \\odot \\sigma_{\\phi}(x_u)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Algorithm I: VAE-SGD\n",
    "\n",
    "<br>$\\textbf{Input}$:Click matrix $X \\, \\epsilon \\, \\mathbb{N}^{I \\times U}$\n",
    "<br>Initialize $\\phi$ and $\\theta$\n",
    "<br>$\\textbf{While}$ $\\textit{not converged}$ $\\textbf{do}$\n",
    "<br>$\\:\\:$sample a batch of users $\\mathcal{U}$\n",
    "<br>$\\:\\:$$\\textbf{for}$ all $u \\, \\epsilon \\, \\mathcal{U}$ $\\textbf{do}$\n",
    "<br>$\\:\\:\\:$sample $\\epsilon \\, \\sim \\, \\mathcal{N}(0, I_k)$ and calculate $z_u$\n",
    "<br>$\\:\\:\\:$compute noisy gradient $\\nabla_{\\phi} \\mathcal{L}$ and $\\nabla_{\\theta} \\mathcal{L}$\n",
    "<br>$\\:\\:$$\\textbf{end}$\n",
    "<br>$\\:\\:$Average the noisy gradients over batch\n",
    "<br>$\\:\\:$Update $\\phi$ and $\\theta$ using gradient steps\n",
    "<br>$\\textbf{end}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.Alternative Interpretation of ELBO\n",
    "\n",
    "We now define a new ELBO which interpretes first term in Eq.(2) as (negative) reconstruction error and the second term, KL term, as the regularization term.\n",
    "\n",
    "$$\\mathcal{L}_{\\beta}(x_u;\\theta, \\phi)=\\mathbb{E}_{q_{\\phi}(z_u|x_u)}[log\\, p_{\\theta}(x_u|z_u)]-\\beta \\, \\dot \\,KL(q_{\\phi}(z_u|x_u)||p(z_u)) $$\n",
    "\n",
    "This regularization view of ELBO trades off between how well we can fit the data and how close the approximate posterior stays to the prior.\n",
    "\n",
    "Selecting $\\textbf{$\\beta$}$: Using KL-annealing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
